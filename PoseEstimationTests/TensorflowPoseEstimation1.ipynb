{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_keypoints(frame, keypoints, edges, confidence):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1,x1,c1 = shaped[p1]\n",
    "        y2,x2,c2 = shaped[p2]\n",
    "        if (c1 > confidence) & (c2 > confidence):\n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2),int(y2)), (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  confidence = confidence threshold\n",
    "def render_keypoints(frame, keypoints, confidence):\n",
    "    y,x,c = frame.shape\n",
    "#     squeeze makes it so you don't have to do keypoints[0][0][0]... when multiple arrays are wrapped\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for keypoint in shaped:\n",
    "        ky, kx, kp_conf = keypoint\n",
    "        if kp_conf > confidence:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0,0,255),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints that connect\n",
    "edges = {\n",
    "    (0,1):'m',\n",
    "    (0,2):'c',\n",
    "    (1,3):'m',\n",
    "    (2,4):'c',\n",
    "    (0,5):'m',\n",
    "    (0,6):'c',\n",
    "    (5,7):'m',\n",
    "    (7,9):'m',\n",
    "    (6,8):'c',\n",
    "    (8,10):'c',\n",
    "    (5,6):'y',\n",
    "    (5,11):'m',\n",
    "    (6,12):'c',\n",
    "    (11,12):'y',\n",
    "    (11,13):'m',\n",
    "    (13,15):'m',\n",
    "    (12,14):'c',\n",
    "    (14,16):'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "#     reshape to 192x192x3 for predictions\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0),192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "#     make predictions\n",
    "    outputs = movenet(input_image)\n",
    "    keypoints = outputs['output_0']\n",
    "    keypoints = np.array(keypoints)\n",
    "    \n",
    "#     render and connect keypoints on original image\n",
    "    connect_keypoints(frame,keypoints,edges,0.4)\n",
    "    render_keypoints(frame,keypoints,0.4)\n",
    "        \n",
    "    cv2.imshow('MoveNet Lightning',frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.31869435, 0.6149604 , 0.578938  ],\n",
       "         [0.28373867, 0.6319184 , 0.78487885],\n",
       "         [0.2896815 , 0.5712187 , 0.58937466],\n",
       "         [0.29884943, 0.6353476 , 0.42747188],\n",
       "         [0.31400707, 0.48757836, 0.59329516],\n",
       "         [0.48371086, 0.70575416, 0.7718184 ],\n",
       "         [0.47461396, 0.418931  , 0.5442802 ],\n",
       "         [0.652882  , 0.8587491 , 0.5514015 ],\n",
       "         [0.41454837, 0.10369816, 0.5412239 ],\n",
       "         [0.7121167 , 0.99589   , 0.17565593],\n",
       "         [0.24730925, 0.30501956, 0.5433493 ],\n",
       "         [0.87306684, 0.7398648 , 0.41355956],\n",
       "         [0.89467096, 0.5097911 , 0.3946021 ],\n",
       "         [0.85224026, 1.0009174 , 0.07488456],\n",
       "         [0.8737728 , 0.4558462 , 0.04254714],\n",
       "         [0.88131344, 0.6757777 , 0.02033579],\n",
       "         [0.25546405, 0.293768  , 0.12477973]]]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints = np.array(keypoints)\n",
    "keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_eye = keypoints[0][0][2]\n",
    "left_elbow = keypoints[0][0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2896815 , 0.5712187 , 0.58937466], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2896815, 0.5712187], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_eye[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([139, 365])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#    480 640 image original dimensions(height, width) for my webcame\n",
    "np.array(right_eye[0:2]*[480,640]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
